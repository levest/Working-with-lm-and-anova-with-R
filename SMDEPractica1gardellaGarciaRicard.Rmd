---
title: "Práctica1SMDE_GardellaGarciaRicard"
author: "Ricard Gardella Garcia"
date: "30/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##FIRST QUESTION: GENERATE A RANDOM SAMPLE.

On this exercise, we are going to start working with probability distributions. The first work to do is to generate more than 200 observations of your assigned probability distribution. 
We are going to generate this using a Spreadsheet.
Now you must import this distribution to R and test the fitting of the values with a new sample now generated by R. Use for the fitting a Chi-square test.

Here, a normal distribution is generated and showed using the "summary" command. We can see the mean of the distribution, the min, the max, the median and the values of the 1st and 3rd quartil. The values of the distribution are random.


```{r generation1}
v1=rnorm(200, mean=0, sd=1)
summary(v1)
```

The data of the normal distribution into a data frame
```{r table1}
taula_v1=data.frame(x1=v1)
```

Here the intervals are defined and the categories to be used aswell.
```{r definitionCategories}
taula_v1_cat=transform(taula_v1, cat = 
                         ifelse(x1 < -1,"-1",
                         ifelse(x1 < -0.5,"-0.5", 
                          ifelse(x1 < 0,"0", 
                          ifelse(x1 < 0.5,"0.5",
                          ifelse(x1 <1,"1","Inf"))))))
```

After this step, an excel spreadsheet has to be generated in order to complete this exercice, then, we must generate random values and then apply the same method as above to fill a new columns with the categories that later will be used to compare both results.

```{r definitionExcelCategories}
library(readxl)
ExcelPract1Estadistica <- read_excel("~/Documents/Master Data science/SMDE/Prácticas/Práctica1/ExcelPract1Estadistica.xlsx")
ExcelPract1Estadistica=transform(ExcelPract1Estadistica, Categoria = 
                         ifelse(SPECIAL < -1,"-1",
                                ifelse(SPECIAL < -0.5,"-0.5", 
                                       ifelse(SPECIAL < 0,"0", 
                                              ifelse(SPECIAL < 0.5,"0.5",
                                                     ifelse(SPECIAL <1,"1","Inf"))))))
```

Now, we count the amount of each categories from both excel and r

```{r countingcategories}
taula_freq_v1=as.data.frame(with(taula_v1_cat, table(cat)))
taula_freq_excel=as.data.frame(with(ExcelPract1Estadistica, table(Categoria)))

```

A table that count the categories of both distributions is created

```{r tablebothdistr}
taula_freq=data.frame(x1=taula_freq_v1[2],x2=taula_freq_excel[2])
```

Then, we can apply the chisquare test. That test give us the x squared and the P value.
```{r chisquaretest}
Test=chisq.test(taula_freq, correct=FALSE)
Test
```

Also, we can represent the excel and r distributions using histograms in order to see the diference in a more visual way.

```{r histogramsex1}
hist(taula_freq[2:6,1], 
     main="Histogram of R frequency", 
     xlab="Frequency")
hist(taula_freq[2:6,2], 
     main="Histogram of Excel frequency", 
     xlab="Frequency")
```

Now will implement the same, but for an exponential distribution also using a exponential distribution in excel.

```{r iniexp}
v2=rexp(200)
ExcelPract1EstadisticaExp <- read_excel("~/Documents/Master Data science/SMDE/Prácticas/Práctica1/ExcelPract1EstadisticaExp.xlsx")
taula_v2=data.frame(x2=v2)
```
Perform the same than before in order to perform the chisquare test. As we can see in the chi-square test, the value is near 0 so the test is correct.

```{r expperform}
taula_v2_cat=transform(taula_v2, cat = 
                         ifelse(x2 < -1,"-1",
                                ifelse(x2 < -0.5,"-0.5", 
                                       ifelse(x2 < 0,"0", 
                                              ifelse(x2 < 0.5,"0.5",
                                                     ifelse(x2 <1,"1","Inf"))))))
ExcelPract1Estadistica=transform(ExcelPract1EstadisticaExp, Categoria = 
                                   ifelse(SPECIAL < -1,"-1",
                                          ifelse(SPECIAL < -0.5,"-0.5", 
                                                 ifelse(SPECIAL < 0,"0", 
                                                        ifelse(SPECIAL < 0.5,"0.5",
                                                               ifelse(SPECIAL <1,"1","Inf"))))))
#Counting the amount of elements in each category “table” function. 
taula_freq_v2=as.data.frame(with(taula_v2_cat, table(cat)))
taula_freq_excel=as.data.frame(with(ExcelPract1Estadistica, table(Categoria)))
#Creation of the table containing the frequencies of both ditributions
taula_freq2=data.frame(x1=taula_freq_v2[2],x2=taula_freq_excel[2])
Test=chisq.test(taula_freq2, correct=FALSE)
Test
```

Now, we will see the histograms for the exponential distribution. As we can see are similar to the normal distribution.

```{r histo1exp}
hist(taula_freq2[2:6,1], 
     main="Histogram of R frequency", 
     xlab="Frequency")
```

```{r histo2exp}
hist(taula_freq2[2:6,2], 
     main="Histogram of Excel frequency", 
     xlab="Frequency")
```


To conlude this part, i will replicate the first part, with a normal distribution, but with a mean of 10, let's see what happen. We will perform the same in order to get the chisquare test. As we can see, the r squared is the double that the one that we obtained in the first test of the exercice, that's why our mean of the rnom is 10, instead of 0. 

```{r}
v3=rnorm(200, mean=10,sd=1)
taula_v3=data.frame(x3=v3)
#Definition of the intervals, categories to be used. 
taula_v3_cat=transform(taula_v3, cat = 
                         ifelse(x3 < -1,"-1",
                                ifelse(x3 < -0.5,"-0.5", 
                                       ifelse(x3 < 0,"0", 
                                              ifelse(x3 < 0.5,"0.5",
                                                     ifelse(x3 <1,"1","Inf"))))))
ExcelPract1Estadistica=transform(ExcelPract1Estadistica, Categoria = 
                                   ifelse(SPECIAL < -1,"-1",
                                          ifelse(SPECIAL < -0.5,"-0.5", 
                                                 ifelse(SPECIAL < 0,"0", 
                                                        ifelse(SPECIAL < 0.5,"0.5",
                                                               ifelse(SPECIAL <1,"1","Inf"))))))
#Counting the amount of elements in each category “table” function. 
taula_freq_v3=as.data.frame(with(taula_v3_cat, table(cat)))
taula_freq_excel3=as.data.frame(with(ExcelPract1Estadistica, table(Categoria)))

#Creation of the table containing the frequencies of both ditributions
taula_freq3=data.frame(x1=taula_freq_v3[2],x2=taula_freq_excel3[2])

#Chisquare test
Test=chisq.test(taula_freq3, correct=FALSE)
Test
```

As we can see below, the data from the excel is the same or very similiar due randomness, but the data from the new random distribution, as his mean is 10, the histogram is not correct. To correct that, we had too change the table in the variable "taula_v3_cat".
```{r histo3}
hist(taula_freq3[2:6,1], 
     main="Histogram of R frequency", 
     xlab="Frequency")
```

```{r histo3excel}
hist(taula_freq3[2:6,2], 
     main="Histogram of Excel frequency", 
     xlab="Frequency")
```

####Conclusions
As we can see in the histograms, the results of the R and Excel distributions are different but also are really similar. But, if we change the distributions, we can get very different results. It is important to use the same distribution either excel and R and also similar means.


##SECOND QUESTION: ANOVA.

Here, three random normal distributions are generated, one with a mean of 10 and the other ones with a mean of 0.

Then, the datais merged and a boxplot is applied. 

Also, we will be testing if the populations from the rnorm are really normal. To see the diference, I've created and exponential population. The p-value of the exponential population is 0.

```{r providedcode}
#Provided code
library(RcmdrMisc)
Norm_v1=rnorm(200, mean=0, sd=1) 
Norm_v2=rnorm(200, mean=10, sd=1) 
Norm_v3=rnorm(200, mean=0, sd=1)
exp = rexp(200, rate=1)

shapiro.test(Norm_v1)
shapiro.test(Norm_v2)
shapiro.test(Norm_v3)
shapiro.test(exp)

Norm_v1n=data.frame(x1=Norm_v1, x2="v1") 
Norm_v2n=data.frame(x1=Norm_v2, x2="v2") 
Norm_v3n=data.frame(x1=Norm_v3, x2="v3") 
data=mergeRows(Norm_v1n, Norm_v2n, common.only=FALSE)
data=mergeRows(as.data.frame(data), Norm_v3n, common.only=FALSE)

AnovaModel1 <- aov(x1 ~ x2, data=data) 
summary(AnovaModel1)
```

What is seen in this boxplot is the representation of the three random normal distributions that we created on the first part of the exercice. 

It is clear that the distribution with mean 10 stays, of course, on the value 10 and the other two stay on the 0, because their mean is 0. 

```{r boxplot}
Boxplot(x1~x2, data=data, id.method="y")
```

This test tests the indepency of the variables in the data set. 
```{r indepedencytest}
lmtest::dwtest(AnovaModel1, alternative = "two.sided")
```



This test, tests the homogenity of the model.
```{r homogenity test}
lmtest::bptest(AnovaModel1)
```

This library is necessary in order to 
```{r libraryanova}
library("lmtest", lib.loc="~/Library/R/3.3/library")
#Now we'll implement the tests using the library lmtest in order eto test the ANOVA
```

First of all, we will test the normality of the populations of the drift realted to block.size.medium. In order to do that, we extract the unique values from geomorphology$drift, and then, subset the data from geomorphology, selecting the correct data realted to the correspondent Drift and performing the test.

As we can see, not all the populations are normal. The Diamict drift population is not normal. So, the asumption for this anova can't be trusted. 

Organic soil have less than 3 variables, so, the test cant be performed on that population. That's very important when performing shapiro tests, because, that's another cause for what this Anova model can't be trusted.
```{r normalitygeo}
data(geomorphology, package="FactoMineR")
uniqueNames = data.frame(unique(geomorphology$Drift))
#Test for each drift population
for(i in 1:nrow(uniqueNames)){
  shapiroData <- subset(geomorphology, Drift==as.character(uniqueNames$unique.geomorphology.Drift.[i]),select = Block.size.median)
  print("Shapiro test on: ")
  print(as.character(uniqueNames$unique.geomorphology.Drift.[i]))
  if(nrow(shapiroData)>3) {
    print(shapiro.test(shapiroData$Block.size.median))
    }
  else{print("Must have 3 or more rows in order to perform the shapiro test.")}
  }
```

Now, the anova model will be created and tested for 2 more assumptions, independency and homogeniety.
```{r creationanova1}
Anova2 <- aov(Block.size.median ~ Drift ,data= geomorphology)
#Independent test
lmtest::dwtest(formula = Anova2,alternative = "two.sided")
#Homogenety tests
lmtest::bptest(Anova2)
```

After that, we can create a boxplot in order to see the data in a more visual way.

We can observe that, we have a Drift value without any value. Thats because that drift only have one value and can't be represented. Landslide, on the other hand, have very few values, but enought

```{r boxplotBlocksize}
Boxplot(Block.size.median ~ Drift, data=geomorphology, id.method="y")

```


Now, we will do the same than before, but, for the population of Drift and Wind.

We will start doing the normality test of the populations. As we can see, now, all the populations are normal so the first asumption 

```{r normalityWind}
for(i in 1:nrow(uniqueNames)){
  shapiroData <- subset(geomorphology, Drift==as.character(uniqueNames$unique.geomorphology.Drift.[i]),select = Wind.effect)
  print("Shapiro test on: ")
  print(as.character(uniqueNames$unique.geomorphology.Drift.[i]))
  if(nrow(shapiroData)>3) {
    print(shapiro.test(shapiroData$Wind.effect))
  }
  else{print("Must have 3 or more rows in order to perform the shapiro test.")}
}
```

Now, like before, we create the anova and do the other two asumptions, as we can see, the Durbin-Watson test give un a p-value of 0.002, lower than 0.05, so, we can't aprobe this assumption, so, the model can't be trusted.

```{r assumptionsWindanova}
Anova3 <- aov(Wind.effect ~ Drift ,data= geomorphology)
#Independent test
lmtest::dwtest(formula = Anova3,alternative = "two.sided")
#Homogenety tests
lmtest::bptest(Anova3)
```

The boxplot shows the population of Drift and Windeffect. As before, a value is missing, thats why, like before, the Drift value "organic soil" only have 1 value, so can not be computed.

```{r boxplotanova3}
Boxplot(Wind.effect ~ Drift, data=geomorphology, id.method="y")
```


##THIRD QUESTION: DEFINE A LINEAR MODEL FOR AN ATHLETE IN THE 1500 M.

For this part of the exericice, a new data frame will be used. In this case, the data frame "Decathlon" will be used.

```{r dataframelinearmodel}
data(decathlon, package="FactoMineR")
```

First of all, we need to change some names of the variables, because, if not, we could not acces these variables correctly.

```{r changenames}
colnames(decathlon)[10] <- "D1500M"
colnames(decathlon)[5] <- "D400M"
colnames(decathlon)[1] <- "D100M"
```

Here we create the liniar model, looking for the regresion between D1500M and all the other variables.

```{r regmodel1}
RegModel1 <- lm(D1500M~., data=decathlon)
RegModel1
```

Here is the summary of the liniar model we just created, as we can see, the model have and R-squared of 0.9975, we will try to improbe this value.

```{r summarymodel1}
summary(RegModel1)
```

Now we do the testing for the regmodel 1. As we can see, the tests are correct, the p-value is higher than 0.05 and the other values make sense.

```{r testingregmodel1}
#Independicy test 
lmtest::dwtest(RegModel1, alternative = "two.sided")
#homogenity test
lmtest::bptest(RegModel1)
#Normality test
shapiro.test(residuals(RegModel1))

```

In order to improbe the r-squared, we will remove some variables that are non quantitative, for example, first, we will remove the rank. As we can see, the value now is 0.9976, it has improbed. We can also see that the asumptions are correct and we can trust the model.

```{r regmodel2}
RegModel2 <- lm(D1500M~. -Rank, data=decathlon)
RegModel2
summary(RegModel2)
plot(RegModel2)
```

```{r testingregmodel2}
#Independicy test 
lmtest::dwtest(RegModel2, alternative = "two.sided")
#homogenity test
lmtest::bptest(RegModel2)
#Normality test
shapiro.test(residuals(RegModel2))

```
Finally, we will do the same but removing also the variable competition for the exact same reason. The R-squared value has been improbed to 0.9977. We can see also that the asumptions are correct, so, we can trust the model

```{r regmodel3}
RegModel3 <- lm(D1500M~. -Rank -Competition, data=decathlon)
RegModel3
summary(RegModel3)
plot(RegModel3)
```

```{r testingregmodel3}
#Independicy test 
lmtest::dwtest(RegModel3, alternative = "two.sided")
#homogenity test
lmtest::bptest(RegModel3)
#Normality test
shapiro.test(residuals(RegModel3))

```
## FOURTH QUESTION: USE THE MODEL TO PREDICT THE BEHAVIOR OF AN ATHLETE.

Now, we want to predict the behaviour of an athelte, in order to do that, we will use the function predict. Those models will predict the behaviour of an athlete, running 1500M, in prediction interval the first one and confidence the second one.

Here, we see predicitons for the model 3, so, it's the prediction of athletes running the 1500M but without the variables rank and competition. 

```{r prediction}
predict(RegModel3, newdata=decathlon, interval="prediction")
predict(RegModel3, newdata=decathlon, interval="confidence")
```

Now, i will repeat the predictons, but changing the model, now, the linear regression is between 1500M and 400M. As we can see, we can't trust the model because the normality test is not passed, because the result of that test is 0.01

```{r}
#Prediction for 400 and 1500M
RegModel4 <- lm(D1500M~D400M, data=decathlon)
#Independicy test 
lmtest::dwtest(RegModel4, alternative = "two.sided")
#homogenity test
lmtest::bptest(RegModel4)
#Normality test
shapiro.test(residuals(RegModel4))
new <- data.frame(decathlon$D400M)
colnames(new)[1] <- "D400M"
predict(RegModel4, newdata=new, interval="prediction")
predict(RegModel4, newdata=new, interval="confidence")
plot(RegModel4)
```

As we can see in this case, we take the rank and we make a linear regresion with D100M, D400M, D1500M and points, the normality and homogenity test are correct, but the independency test is not correct, so, we can not trust this model.

```{r lmmodel5}
RegModel5 <- lm(Rank~ D100M + D400M + D1500M + Points, data=decathlon)
#Independicy test 
lmtest::dwtest(RegModel5, alternative = "two.sided")
#homogenity test
lmtest::bptest(RegModel5)
#Normality test
shapiro.test(residuals(RegModel5))
new <- data.frame(decathlon$D100M,decathlon$D400M,decathlon$D1500M,decathlon$Points)
colnames(new)[1] <- "D100M"
colnames(new)[2] <- "D400M"
colnames(new)[3] <- "D1500M"
colnames(new)[4] <- "Points"
predict(RegModel5, newdata=new, interval="prediction")
predict(RegModel5, newdata=new, interval="confidence")
plot (RegModel5)
```


##FIVE QUESTION: PCA

The final part is to create a PCA, in order to do that we will use the function PCA, but, we will delete the variables Rank and Competition for the same reason than before, for being qualitative data intead of quantitative. The library FactoMineR is mandatory in order to compute the PCA.

```{r PCA}
library(FactoMineR)
pcaData <- subset( decathlon, select = -c(Competition,Rank,Points))
PCA(pcaData)
```


##CONCLUSIONS

In this asignment we have seen some diferent distributions and forms of expresing the data depending of what we are looking for. 

One of the main points that have been covered is the mandatory and high importance of the assumptions on the models. 

All the models have to be tested, because, without the tests, we can not know if that model can be trusted or not. It is true that most of the times, some tests are not done, for example, independency test is the one that we can be more flexible with, but, it is also important to have in mind that if a model do not pass the independency test, some problems can appear later on.

Also, the addition of excel in this asignment makes itself more interesting, also proves, that excel is a very usefull tool that a lot of companies use. Excel also is a very common way to provide us data to analyse, because a lot of companies use excels as a main data structure and analysis.





